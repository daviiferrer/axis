const { GoogleGenerativeAI, HarmBlockThreshold, HarmCategory } = require("@google/generative-ai");
const CircuitBreaker = require('opossum');
const logger = require('../../shared/Logger').createModuleLogger('gemini');
const { updateTraceContext } = require('../../shared/TraceContext');
const SpintaxService = require('../../core/services/content/SpintaxService');
const ModelPricingService = require('../../core/services/ai/ModelPricing');

/**
 * Safety settings for sales/customer service AI.
 * We need to allow processing offensive USER messages (leads being rude)
 * while still blocking the AI from generating harmful content.
 * BLOCK_NONE allows processing all input - the AI still won't generate bad content.
 */
const SALES_SAFETY_SETTINGS = [
    { category: HarmCategory.HARM_CATEGORY_HARASSMENT, threshold: HarmBlockThreshold.BLOCK_NONE },
    { category: HarmCategory.HARM_CATEGORY_HATE_SPEECH, threshold: HarmBlockThreshold.BLOCK_NONE },
    { category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, threshold: HarmBlockThreshold.BLOCK_NONE },
    { category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold: HarmBlockThreshold.BLOCK_NONE },
];

/**
 * GeminiClient - Infrastructure Client for Google Gemini AI
 * 
 * O modelo a ser usado DEVE ser passado explicitamente em cada chamada.
 * A configuraÃ§Ã£o do modelo vem da tabela `agents` no banco de dados.
 * NÃƒO hÃ¡ modelo padrÃ£o hardcoded - se nÃ£o for passado, lanÃ§a erro.
 */
class GeminiClient {
    constructor({ billingService, settingsService }) {
        this.billingService = billingService;
        this.settingsService = settingsService;
        this.apiKey = null;
        this.genAI = null;

        // Circuit Breaker configuration
        // More tolerant settings for production - don't open too quickly
        this.breakerOptions = {
            timeout: 15000,              // 15s timeout for Gemini calls
            errorThresholdPercentage: 70, // Open after 70% failures (was 50%)
            resetTimeout: 30000,          // Try again after 30 seconds
            volumeThreshold: 10           // Need 10 requests before calculating error rate (was 5)
        };

        this.generateBreaker = new CircuitBreaker(
            this._rawGenerateContent.bind(this),
            this.breakerOptions
        );

        this.generateBreaker.fallback(() => {
            logger.warn('Circuit breaker triggered - using fallback message');
            const fallbackJson = JSON.stringify({
                response: SpintaxService.getFallbackMessage(),
                thought: "System Overload / Circuit Open",
                sentiment_score: 0.5,
                confidence_score: 0.0
            });
            return {
                text: () => fallbackJson,
                _metrics: { fallback: true }
            };
        });

        this.generateBreaker.on('open', () => logger.warn('âš ï¸ Circuit OPEN - Gemini unavailable'));
        this.generateBreaker.on('halfOpen', () => logger.info('ðŸ”„ Circuit HALF-OPEN - testing Gemini'));
        this.generateBreaker.on('close', () => logger.info('âœ… Circuit CLOSED - Gemini recovered'));

        this.simpleBreaker = new CircuitBreaker(
            this._rawGenerateSimple.bind(this),
            this.breakerOptions
        );

        this.simpleBreaker.fallback(() => {
            logger.warn('Circuit breaker triggered (Simple) - using fallback message');
            const fallbackJson = JSON.stringify({
                response: SpintaxService.getFallbackMessage(),
                thought: "System Overload / Circuit Open",
                sentiment_score: 0.5,
                confidence_score: 0.0
            });
            return {
                text: () => fallbackJson,
                _metrics: { fallback: true }
            };
        });

        // Log actual errors that cause circuit breaker to trigger
        this.simpleBreaker.on('failure', (error) => {
            logger.error({ error: error?.message || error, stack: error?.stack?.slice(0, 200) }, 'âŒ Gemini Simple call failed');
        });
        this.generateBreaker.on('failure', (error) => {
            logger.error({ error: error?.message || error, stack: error?.stack?.slice(0, 200) }, 'âŒ Gemini Generate call failed');
        });
    }

    async _handleBilling(metrics, context = {}) {
        if (!this.billingService || !context.companyId) return;

        // Cost Model:
        // Simple Generation: 1 Credit
        // Complex (Chat/Stream): 2 Credits
        const cost = context.isComplex ? 2 : 1;

        try {
            await this.billingService.deductCredits(context.companyId, cost, {
                purpose: 'ai_generation',
                model: metrics.model
            });
        } catch (e) {
            logger.error({ error: e.message }, 'Billing Deduction Failed');
        }
    }

    /**
     * Valida que o cliente estÃ¡ pronto e o modelo foi fornecido.
     * O modelo deve vir da tabela `agents.model` no banco de dados.
     */
    /**
     * Ensures the client is initialized with an API Key for the specific user.
     */
    async _ensureClient(modelName, methodName, context = {}) {
        const userId = context.userId;

        if (!userId) {
            const error = `[GeminiClient] ERRO CRÃTICO: UsuÃ¡rio nÃ£o identificado para contexto de IA. impossÃ­vel carregar chave.`;
            logger.error(error);
            throw new Error(error);
        }

        // 1. Try to load user-specific key (Priority)
        if (this.settingsService) {
            const userKey = await this.settingsService.getProviderKey(userId, 'gemini');
            if (userKey) {
                // Return a new instance for this request (or cache it)
                // For now, we instantiate a lightweight client just for this call context if needed,
                // OR we update this.genAI if we are incorrectly using Singleton.
                // ideally we should return the client instance, not just set this.genAI

                // Hack for now: Re-instantiate this.genAI if key changes or just use it.
                // But this class is likely a Singleton/Scoped in Awilix.
                // If Scoped: it's fine. If Singleton: this is bad (race condition).
                // Assuming Scoped per Request:
                this.apiKey = userKey;
                this.genAI = new GoogleGenerativeAI(userKey);
                // logger.debug({ userId }, 'GeminiClient: Switched to User API Key');
                return modelName;
            }
        }

        // 2. Fallback? NO. User explicitly requested NO FALLBACK.
        // If we don't have a specific key, we check if we have a "global" key loaded via constructor?
        // But constructor loading was from ENV. 
        // If Env key is present, we might still use it IF the user didn't forbid it?
        // User said: "se nÃ£o tiver chave de api o sistema simplesmente nÃ£o cria o agente".
        // This implies logic in AgentService.
        // Here, if we are running, and no key found -> ERROR.

        if (!this.genAI && !this.apiKey) {
            const error = `[GeminiClient] ERRO CRÃTICO: Nenhuma API Key encontrada para o usuÃ¡rio ${userId || 'desconhecido'}. Configure sua chave em ConfiguraÃ§Ãµes.`;
            logger.error(error);
            throw new Error(error);
        }

        // If we have a globally loaded key (from previous loose env config), use it?
        // The user effectively wants to KILL env usage. 
        // But I'll leave the constructor check as a "System Admin" override if ever needed, 
        // but since I removed it safely from this specific method flow, it relies on what's set.

        if (!modelName) {
            const error = `[GeminiClient.${methodName}] ERRO: Modelo nÃ£o especificado.`;
            logger.error(error);
            throw new Error(error);
        }

        return modelName;
    }



    /**
     * Sanitizes chat history for Gemini SDK requirements.
     * 1. Maps 'assistant' role to 'model'
     * 2. Ensures history starts with a 'user' message
     * 3. Ensures strictly alternating roles (user -> model -> user)
     * 4. Merges consecutive messages of the same role
     */
    _sanitizeHistory(history) {
        if (!history || !Array.isArray(history) || history.length === 0) return [];

        const sanitized = [];
        let nextExpectedRole = 'user';

        for (const msg of history) {
            // Map roles
            const geminiRole = (msg.role === 'assistant' || msg.role === 'model') ? 'model' : 'user';

            // Gemini REQUIRES history to start with 'user'.
            // If the AI started the conversation (first msg is 'model'), 
            // we prepend a virtual user message to maintain context without breaking the SDK.
            if (sanitized.length === 0 && geminiRole !== 'user') {
                sanitized.push({
                    role: 'user',
                    parts: [{ text: '[SISTEMA: InÃ­cio da conversa]' }]
                });
                nextExpectedRole = 'model';
            }

            if (geminiRole === nextExpectedRole) {
                sanitized.push({
                    role: geminiRole,
                    parts: msg.parts || [{ text: msg.content || '' }]
                });
                nextExpectedRole = geminiRole === 'user' ? 'model' : 'user';
            } else if (sanitized.length > 0) {
                // Merge same-role consecutive messages (user, user) or (model, model)
                const last = sanitized[sanitized.length - 1];
                const newText = msg.content || msg.parts?.[0]?.text || '';
                if (newText) {
                    last.parts[0].text = (last.parts[0].text || '') + '\n' + newText;
                }
            }
        }
        return sanitized;
    }

    async _rawGenerateContent(modelName, systemInstruction, history, options = {}) {
        const start = performance.now();
        const model = await this._ensureClient(modelName, '_rawGenerateContent', options);

        const genModel = this.genAI.getGenerativeModel({
            model,
            systemInstruction,
            safetySettings: SALES_SAFETY_SETTINGS,
            ...options
        });

        // Gemini history constraints
        const sanitizedHistory = this._sanitizeHistory(history);
        const lastMessage = history[history.length - 1];
        const lastParts = lastMessage.parts || [{ text: lastMessage.content || '' }];

        const chat = genModel.startChat({ history: sanitizedHistory });
        const result = await chat.sendMessage(lastParts);
        const response = result.response;
        const usage = response.usageMetadata || {};

        const total_ms = Math.round(performance.now() - start);
        const metrics = {
            model,
            total_ms,
            prompt_tokens: usage.promptTokenCount || 0,
            completion_tokens: usage.candidatesTokenCount || 0
        };

        logger.info({ metrics }, 'Gemini generation completed');
        response._metrics = metrics;

        if (options.companyId || options.userId) {
            this._handleBilling(metrics, { companyId: options.companyId, isComplex: true });
            this._logUsage(metrics, options);
        }

        return response;
    }

    async _rawGenerateSimple(modelName, systemInstruction, prompt, options = {}) {
        const start = performance.now();
        const model = await this._ensureClient(modelName, '_rawGenerateSimple', options);

        const genModel = this.genAI.getGenerativeModel({
            model,
            systemInstruction,
            safetySettings: SALES_SAFETY_SETTINGS,
            ...options
        });

        const result = await genModel.generateContent(prompt);
        const response = result.response;
        const usage = response.usageMetadata || {};

        const total_ms = Math.round(performance.now() - start);
        const metrics = {
            model,
            total_ms,
            prompt_tokens: usage.promptTokenCount || 0,
            completion_tokens: usage.candidatesTokenCount || 0
        };

        logger.info({ model, total_ms, metrics }, 'Simple generation completed');
        response._metrics = metrics;

        // BILLING & LOGGING HOOK
        if (options.companyId || options.userId) {
            this._handleBilling(metrics, { companyId: options.companyId, isComplex: false });
            this._logUsage(metrics, options);
        }

        return response;
    }

    async generateContent(modelName, systemInstruction, history, options = {}) {
        return this.generateBreaker.fire(modelName, systemInstruction, history, options);
    }

    async generateContentStream(modelName, systemInstruction, history, options = {}) {
        const start = performance.now();
        const model = await this._ensureClient(modelName, 'generateContentStream', options);
        let ttft = null;
        let chunkCount = 0;

        const genModel = this.genAI.getGenerativeModel({
            model,
            systemInstruction,
            ...options
        });

        const sanitizedHistory = this._sanitizeHistory(history);
        const lastMessage = history[history.length - 1];
        const lastParts = lastMessage.parts || [{ text: lastMessage.content || '' }];

        const chat = genModel.startChat({ history: sanitizedHistory });
        const result = await chat.sendMessageStream(lastParts);
        const originalStream = result.stream;

        result.stream = (async function* () {
            for await (const chunk of originalStream) {
                if (ttft === null) {
                    ttft = Math.round(performance.now() - start);
                    logger.debug({ ttft_ms: ttft }, 'Time to first token');
                }
                chunkCount++;
                yield chunk;
            }

            const total_ms = Math.round(performance.now() - start);
            const tpot = chunkCount > 1 ? Math.round((total_ms - ttft) / (chunkCount - 1)) : 0;

            // Wait for the full response to get accurate usage metadata
            const fullResponse = await result.response;
            const usage = fullResponse.usageMetadata || {};

            const metrics = {
                model,
                ttft_ms: ttft,
                tpot_ms: tpot,
                total_ms,
                token_count: chunkCount, // chunks
                prompt_tokens: usage.promptTokenCount || 0,
                completion_tokens: usage.candidatesTokenCount || 0
            };

            logger.info({ metrics }, 'Gemini stream completed');
            updateTraceContext({ cognitive_metrics: metrics });
        })();

        return result;
    }

    async generateSimple(modelName, systemInstruction, prompt, options = {}) {
        return this.simpleBreaker.fire(modelName, systemInstruction, prompt, options);
    }

    /**
     * Generate content with vision (image analysis).
     * Supports sending images alongside text prompts for multimodal AI processing.
     *
     * @param {string} modelName - Gemini model (e.g., 'gemini-2.0-flash')
     * @param {string} systemInstruction - System prompt
     * @param {string} textPrompt - Text prompt/question about the image
     * @param {Array} mediaItems - Array of { mimeType: string, data: string (base64) }
     * @param {object} options - Additional options (userId, campaignId, etc.)
     */
    async generateWithVision(modelName, systemInstruction, textPrompt, mediaItems = [], options = {}) {
        const start = performance.now();
        const model = await this._ensureClient(modelName, 'generateWithVision', options);

        const genModel = this.genAI.getGenerativeModel({
            model,
            systemInstruction,
            safetySettings: SALES_SAFETY_SETTINGS,
        });

        // Build multimodal parts: images first, then text
        const parts = [];
        for (const item of mediaItems) {
            parts.push({
                inlineData: {
                    mimeType: item.mimeType,
                    data: item.data // base64 string
                }
            });
        }
        parts.push({ text: textPrompt });

        const result = await genModel.generateContent(parts);
        const response = result.response;
        const usage = response.usageMetadata || {};

        const total_ms = Math.round(performance.now() - start);
        const metrics = {
            model,
            total_ms,
            prompt_tokens: usage.promptTokenCount || 0,
            completion_tokens: usage.candidatesTokenCount || 0
        };

        logger.info({ model, total_ms, mediaCount: mediaItems.length }, 'ðŸ–¼ï¸ Vision generation completed');
        response._metrics = metrics;

        // BILLING & LOGGING HOOK
        if (options.companyId || options.userId) {
            this._handleBilling(metrics, { companyId: options.companyId, isComplex: true });
            this._logUsage(metrics, options);
        }

        return response;
    }

    async getEmbedding(modelName, text, options = {}) {
        const start = performance.now();
        const model = await this._ensureClient(modelName, 'getEmbedding', options);

        const genModel = this.genAI.getGenerativeModel({ model });
        const result = await genModel.embedContent(text);

        const total_ms = Math.round(performance.now() - start);
        logger.debug({ model, total_ms, text_length: text.length }, 'Embedding generated');

        return result.embedding.values;
    }


    getCircuitStats() {
        return this.generateBreaker.stats;
    }



    /**
     * Logs Token Usage to Supabase (Async - Fire & Forget)
     * Supports Granular SaaS Tracking (Campaign, Chat, User)
     */
    async _logUsage(metrics, context = {}) {
        if (!this.settingsService?.supabase) return; // Need supabase client access

        const { companyId, campaignId, chatId, userId, sessionId, leadId } = context;
        if (!userId && !companyId && !campaignId) return; // Need an owner or campaign context

        try {
            // UUID Validation regex
            const isUUID = (str) => /^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$/i.test(str);

            // Calculate Cost using 2026 Real Pricing
            const cost = ModelPricingService.calculateCost(
                metrics.model,
                metrics.prompt_tokens || 0,
                metrics.completion_tokens || 0
            );

            // ADAPTED TO ai_usage_logs TABLE
            const logEntry = {
                user_id: isUUID(userId) ? userId : null,
                campaign_id: isUUID(campaignId) ? campaignId : null,
                chat_id: isUUID(chatId) ? chatId : null,
                session_id: isUUID(sessionId) ? sessionId : null,
                model: metrics.model,
                tokens_input: metrics.prompt_tokens || 0,
                tokens_output: metrics.completion_tokens || 0,
                cost: cost, // In ai_usage_logs it's 'cost', not 'cost_usd'
                created_at: new Date().toISOString()
            };

            const { error } = await this.settingsService.supabase.from('ai_usage_logs').insert(logEntry);

            if (error) {
                logger.error({ error: error.message, model: metrics.model }, 'Supabase AI usage insert failed (ai_usage_logs)');
            } else {
                logger.debug({ model: metrics.model, cost }, 'AI usage logged successfully in ai_usage_logs');
            }
        } catch (e) {
            logger.error({ error: e.message }, 'Failed to log AI usage');
        }
    }
}

module.exports = GeminiClient;

